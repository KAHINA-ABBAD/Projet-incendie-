{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "254c0d37",
   "metadata": {},
   "source": [
    "# Projet Incendies — Notebook Nettoyage, EDA & Modélisation\n",
    "**Dernière génération :** 2025-09-12 10:57 UTC\n",
    "\n",
    "Ce notebook fournit un pipeline **complet et documenté** pour :\n",
    "1) Explorer une base **SQLite** (structure + contenu),\n",
    "2) **Nettoyer** les données avec des décisions guidées par des métriques (manquants, doublons, outliers, catégories rares),\n",
    "3) Réaliser une **EDA** (analyses descriptives & graphiques),\n",
    "4) Construire un **modèle de classification de base** (Random Forest) pour prédire le risque d’incendie **majeur**.\n",
    "\n",
    "> ⚠️ Adapte au besoin le chemin de la base (`db_path`) et le nom de table (`table_name`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6468c613",
   "metadata": {},
   "source": [
    "## 0) Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8814e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports ===\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ne pas utiliser seaborn par défaut pour limiter les dépendances et respecter des consignes stricte\n",
    "# (Tu peux l'activer si tu le souhaites)\n",
    "# import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, RocCurveDisplay\n",
    "\n",
    "# === Paramètres généraux ===\n",
    "pd.set_option(\"display.max_columns\", 120)\n",
    "pd.set_option(\"display.width\", 140)\n",
    "plt.rcParams['figure.figsize'] = (9,5)\n",
    "\n",
    "# === Chemins / Tables ===\n",
    "db_path = \"data/incendies.db\"      # ← adapte si besoin\n",
    "table_name = \"incendies\"           # ← adapte si besoin\n",
    "\n",
    "Path(db_path).exists(), db_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9137c1d0",
   "metadata": {},
   "source": [
    "## 1) Inspecter la base SQLite (structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2cb0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect(db_path)\n",
    "\n",
    "# Liste des tables\n",
    "tables = pd.read_sql(\"\"\"\n",
    "SELECT name AS table_name\n",
    "FROM sqlite_master\n",
    "WHERE type='table' AND name NOT LIKE 'sqlite_%'\n",
    "ORDER BY name;\n",
    "\"\"\", con)\n",
    "tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b238f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre de lignes par table + nb de colonnes (approx via PRAGMA)\n",
    "def table_shape_sqlite(connection):\n",
    "    out = []\n",
    "    for t in tables['table_name'].tolist():\n",
    "        n = pd.read_sql(f\"SELECT COUNT(*) AS n FROM {t};\", connection).iloc[0,0]\n",
    "        cols = pd.read_sql(f\"PRAGMA table_info({t});\", connection)\n",
    "        out.append({\"table\": t, \"rows\": n, \"cols\": len(cols)})\n",
    "    return pd.DataFrame(out).sort_values(\"rows\", ascending=False)\n",
    "\n",
    "table_shapes = table_shape_sqlite(con)\n",
    "table_shapes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5263bde",
   "metadata": {},
   "source": [
    "## 2) Charger la table principale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70026c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger un échantillon rapide (si table très grosse) puis plein si OK\n",
    "sample_rows = 200_000  # ajuste si besoin\n",
    "try:\n",
    "    df = pd.read_sql(f\"SELECT * FROM {table_name} LIMIT {sample_rows};\", con)\n",
    "except Exception as e:\n",
    "    print(\"Erreur de lecture : \", e)\n",
    "    raise\n",
    "\n",
    "print(\"Shape (sample):\", df.shape)\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf32120f",
   "metadata": {},
   "source": [
    "## 3) Normaliser les noms de colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e640ccad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_cols(columns):\n",
    "    repl = ((\" \", \"_\"), (\"é\",\"e\"), (\"è\",\"e\"), (\"ê\",\"e\"), (\"à\",\"a\"), (\"ô\",\"o\"), (\"û\",\"u\"), (\"(\",\"\"), (\")\",\"\"))\n",
    "    out = []\n",
    "    for c in columns:\n",
    "        cc = c.strip().lower()\n",
    "        for a,b in repl:\n",
    "            cc = cc.replace(a,b)\n",
    "        out.append(cc)\n",
    "    return out\n",
    "\n",
    "original_cols = df.columns.tolist()\n",
    "df.columns = normalize_cols(df.columns)\n",
    "df.columns[:10], len(df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea72b1a",
   "metadata": {},
   "source": [
    "> **Note :** on garde une trace du mapping `original → normalisé` pour documenter le schéma si besoin :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9362d7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_map = {o:n for o,n in zip(original_cols, df.columns)}\n",
    "pd.DataFrame(col_map.items(), columns=[\"original\",\"normalise\"]).head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c431843",
   "metadata": {},
   "source": [
    "## 4) Parsing des dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f293ea8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essaie de détecter une colonne date plausible\n",
    "date_candidates = [c for c in df.columns if 'date' in c or 'alerte' in c]\n",
    "date_candidates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff5bd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_col = None\n",
    "for c in date_candidates:\n",
    "    try:\n",
    "        tmp = pd.to_datetime(df[c], errors=\"raise\")\n",
    "        date_col = c\n",
    "        break\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "if date_col is None and len(date_candidates):\n",
    "    # fallback permissif\n",
    "    date_col = date_candidates[0]\n",
    "    df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "elif date_col:\n",
    "    df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "\n",
    "date_col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666c5995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraire calendrier si date présente\n",
    "if date_col:\n",
    "    df['annee'] = df[date_col].dt.year\n",
    "    df['mois']  = df[date_col].dt.month\n",
    "    df['jour']  = df[date_col].dt.day\n",
    "    df['jour_annee'] = df[date_col].dt.dayofyear\n",
    "df[[date_col, 'annee','mois','jour']].head(3) if date_col else df.head(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969a2852",
   "metadata": {},
   "source": [
    "## 5) Identifier numériques / catégorielles / surfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ea1394",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = [c for c in df.columns if c not in num_cols]\n",
    "\n",
    "# Colonnes surfaces\n",
    "surface_cols = [c for c in df.columns if 'surface' in c]\n",
    "num_cols[:10], cat_cols[:10], surface_cols[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09db0f91",
   "metadata": {},
   "source": [
    "## 6) Diagnostic qualité — métriques pour décider (ne rien supprimer à l’aveugle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437db8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_report(dataframe):\n",
    "    miss = dataframe.isna().sum().sort_values(ascending=False)\n",
    "    ratio = (dataframe.isna().mean()*100).sort_values(ascending=False)\n",
    "    return pd.DataFrame({'manquants': miss, 'pourcentage': ratio}).reset_index(names='col')\n",
    "\n",
    "def duplicate_report(dataframe):\n",
    "    n_dup = dataframe.duplicated().sum()\n",
    "    return pd.DataFrame({'doublons':[n_dup], 'pourcentage':[100*n_dup/len(dataframe) if len(dataframe) else 0]})\n",
    "\n",
    "def outlier_report_iqr(series):\n",
    "    s = pd.to_numeric(series, errors='coerce').dropna()\n",
    "    if s.empty:\n",
    "        return {'q1':np.nan,'q3':np.nan,'iqr':np.nan,'borne_sup':np.nan,'outliers':0,'pct':0.0}\n",
    "    q1, q3 = s.quantile([0.25, 0.75])\n",
    "    iqr = q3 - q1\n",
    "    borne_sup = q3 + 1.5*iqr\n",
    "    out = (s > borne_sup).sum()\n",
    "    return {'q1':q1,'q3':q3,'iqr':iqr,'borne_sup':borne_sup,'outliers':out,'pct':100*out/len(s)}\n",
    "\n",
    "def category_report(series, top_n=20):\n",
    "    vc = series.astype('string').fillna('<NA>').value_counts(dropna=False, normalize=True)*100\n",
    "    return vc.head(top_n).rename('pct').to_frame()\n",
    "\n",
    "# Rapports\n",
    "miss_df = missing_report(df)\n",
    "dup_df  = duplicate_report(df)\n",
    "miss_df.head(15), dup_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff5530d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rapport d'outliers (IQR) pour les colonnes numériques principales (dont surfaces si numériques)\n",
    "iqr_rows = []\n",
    "for c in (surface_cols if surface_cols else num_cols):\n",
    "    stats = outlier_report_iqr(df[c])\n",
    "    stats['col'] = c\n",
    "    iqr_rows.append(stats)\n",
    "iqr_df = pd.DataFrame(iqr_rows).sort_values('pct', ascending=False)\n",
    "iqr_df.head(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba89168",
   "metadata": {},
   "source": [
    "> **Interprétation (règles pratiques)**  \n",
    "- **Valeurs manquantes** :  \n",
    "  - `< 5%` → imputation simple (0, médiane, mode) acceptable.  \n",
    "  - `5–30%` → à discuter, imputation prudente.  \n",
    "  - `> 30%` → envisager de supprimer la colonne **non critique**.  \n",
    "- **Doublons** :  \n",
    "  - `< 1%` → suppression OK.  \n",
    "  - `> 10%` → probable problème amont (ingestion / jointures).  \n",
    "- **Outliers (IQR)** :  \n",
    "  - `< 1%` → capping/suppression possible.  \n",
    "  - `> 5%` → peut refléter des cas extrêmes **réels** (à garder)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091c0657",
   "metadata": {},
   "source": [
    "## 7) Paramètres & Actions de Nettoyage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccc5669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Paramètres de décision ===\n",
    "MISSING_DROP_THRESHOLD = 30.0   # % manquants au-delà duquel on drop (si non critique)\n",
    "OUTLIER_CAP = True              # True = on tronque au seuil IQR\n",
    "FILL_SURFACES_WITH_ZERO = True  # surfaces NaN → 0\n",
    "FILL_CATEG_MODE = True          # catégorielles NaN → mode\n",
    "\n",
    "# === 7.1) Traiter les colonnes de surfaces ===\n",
    "for c in surface_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "if FILL_SURFACES_WITH_ZERO and surface_cols:\n",
    "    df[surface_cols] = df[surface_cols].fillna(0)\n",
    "    for c in surface_cols:\n",
    "        df.loc[df[c] < 0, c] = 0  # pas de valeurs négatives logiques\n",
    "\n",
    "# === 7.2) Imputer certaines catégorielles par le mode ===\n",
    "if FILL_CATEG_MODE and cat_cols:\n",
    "    for c in cat_cols:\n",
    "        mode_val = df[c].mode(dropna=True)\n",
    "        if len(mode_val):\n",
    "            df[c] = df[c].fillna(mode_val[0]).astype(str).str.strip().str.lower()\n",
    "\n",
    "# === 7.3) Supprimer colonnes trop manquantes (non critiques) ===\n",
    "cols_to_drop = miss_df.loc[miss_df['pourcentage']>MISSING_DROP_THRESHOLD, 'col'].tolist()\n",
    "# Protéger des colonnes critiques si tu en as (ex: identifiants, dates, codes INSEE)\n",
    "critical_cols = [date_col] if date_col else []\n",
    "cols_to_drop = [c for c in cols_to_drop if c not in critical_cols]\n",
    "\n",
    "df.drop(columns=list(set(cols_to_drop)), inplace=True, errors='ignore')\n",
    "cols_to_drop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce71138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 7.4) Capping des outliers (IQR) ===\n",
    "if OUTLIER_CAP:\n",
    "    for c in (surface_cols if surface_cols else num_cols):\n",
    "        stats = outlier_report_iqr(df[c])\n",
    "        if not np.isnan(stats['borne_sup']):\n",
    "            df.loc[df[c] > stats['borne_sup'], c] = stats['borne_sup']\n",
    "\n",
    "# === 7.5) Supprimer les doublons ===\n",
    "n_dup = df.duplicated().sum()\n",
    "df = df.drop_duplicates()\n",
    "n_dup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d1fb3f",
   "metadata": {},
   "source": [
    "## 8) Variables dérivées utiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635c484c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total des surfaces brûlées\n",
    "if surface_cols:\n",
    "    df['surface_totale'] = df[surface_cols].sum(axis=1)\n",
    "else:\n",
    "    df['surface_totale'] = np.nan\n",
    "\n",
    "# Cible binaire : incendie majeur (> 1000 m²)\n",
    "df['incendie_majeur'] = (df['surface_totale'] > 1000).astype(int)\n",
    "\n",
    "# Saisonnalité cyclique (si mois dispo)\n",
    "if 'mois' in df.columns:\n",
    "    df['sin_mois'] = np.sin(2*np.pi*df['mois']/12)\n",
    "    df['cos_mois'] = np.cos(2*np.pi*df['mois']/12)\n",
    "\n",
    "df[['surface_totale','incendie_majeur']].head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0abd3f",
   "metadata": {},
   "source": [
    "## 9) EDA — Explorations rapides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd8a2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.1) Nombre d'incendies par année\n",
    "if 'annee' in df.columns:\n",
    "    fires_per_year = df.groupby('annee').size()\n",
    "    ax = fires_per_year.plot(marker='o')\n",
    "    ax.set_title(\"Nombre d'incendies par année\")\n",
    "    ax.set_xlabel(\"Année\")\n",
    "    ax.set_ylabel(\"Nombre\")\n",
    "    ax.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# 9.2) Distribution des surfaces totales (log)\n",
    "if 'surface_totale' in df.columns:\n",
    "    ax = df['surface_totale'].plot(kind='hist', bins=60, log=True)\n",
    "    ax.set_title(\"Distribution des surfaces totales (log)\")\n",
    "    ax.set_xlabel(\"Surface (m²)\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bad9ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.3) Top départements (si colonne dispo)\n",
    "dept_col_candidates = [c for c in df.columns if c in ['departement','code_departement','dept','code_dept']]\n",
    "if dept_col_candidates:\n",
    "    dcol = dept_col_candidates[0]\n",
    "    top_dept = df[dcol].astype(str).value_counts().head(15)\n",
    "    ax = top_dept.plot(kind='bar')\n",
    "    ax.set_title(f\"Top départements par nombre d'enregistrements ({dcol})\")\n",
    "    ax.set_ylabel(\"Nombre\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a426c0",
   "metadata": {},
   "source": [
    "## 10) Modélisation — Baseline Random Forest (classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed9ee60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Features ===\n",
    "feature_cols = []\n",
    "if surface_cols:\n",
    "    feature_cols += surface_cols\n",
    "for c in ['annee','mois','sin_mois','cos_mois']:\n",
    "    if c in df.columns:\n",
    "        feature_cols.append(c)\n",
    "\n",
    "# Sélection et nettoyage final\n",
    "X = df[feature_cols].copy().fillna(0)\n",
    "y = df['incendie_majeur'].copy() if 'incendie_majeur' in df.columns else None\n",
    "\n",
    "X.shape, y.value_counts(normalize=True) if y is not None else \"Pas de cible\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85778e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train/test (stratifié si possible)\n",
    "if y is not None and y.nunique() == 2 and len(X) > 0:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=250, random_state=42, n_jobs=-1)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "    y_proba = rf.predict_proba(X_test)[:,1] if hasattr(rf, 'predict_proba') else None\n",
    "\n",
    "    print(\"=== Rapport de classification ===\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Matrice de confusion\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(\"Matrice de confusion:\\n\", cm)\n",
    "\n",
    "    # ROC-AUC si proba dispo\n",
    "    if y_proba is not None:\n",
    "        auc = roc_auc_score(y_test, y_proba)\n",
    "        print(f\"ROC-AUC: {auc:.3f}\")\n",
    "        RocCurveDisplay.from_predictions(y_test, y_proba)\n",
    "        plt.show()\n",
    "\n",
    "    # Importances\n",
    "    imp = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "    display(imp.head(15))\n",
    "    ax = imp.head(20).sort_values().plot(kind='barh')\n",
    "    ax.set_title(\"Top importances — RandomForest\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Cible indisponible ou données insuffisantes pour l'apprentissage.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fc978a",
   "metadata": {},
   "source": [
    "## 11) Pistes d'amélioration\n",
    "- **Validation géographique** : entraîner sur des départements et tester sur d’autres.\n",
    "- **Enrichissement** : ajouter météo (vent, température, sécheresse), végétation (Corine Land Cover), altitude.\n",
    "- **Modèles** : Gradient Boosting (XGBoost/LightGBM), calibration des probabilités, courbe PR.\n",
    "- **Clustering spatio-temporel** : DBSCAN/HDBSCAN pour hotspots + saisonnalité (features cycliques plus fines).\n",
    "- **App Streamlit** : deux onglets *Historique* et *Prédiction* pour une démonstration end‑to‑end.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
